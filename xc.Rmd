---
title: "Final Assignment"
author: "Philipp Habicht"
date: "2022-11-11"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tidyverse)
library(nnet)
library(e1071)
library(ggplot2)
library(caret)
library(randomForest)
library(blorr)
library(xgboost)
library(ggplot2)
library(ggpubr)
```

```{r ex}
# Read in data from excel
data = read_xlsx("/Users/hawk/Documents/Unimelb/Practice of Statistics and Data Science/assignment5/artery-1.xlsx")

# Cleaning and summarizing
# Grouping x and y variables
x = data.frame(data$Age, data$Gender, data$Diabetes, data$`Ever smoked`, 
               data$PVD, data$CVD)
colnames(x) = c("Age", "Gender", "Diabetes", "Ever_smoked", "PVD", "CVD")
y = data.frame(data$`RA medial calcification`, data$`ITA intimal abnormality`)
colnames(y) = c("RA", "ITA")

# Summarize data
summary(x)
summary(y)

# Classification problem: Building 4 groups from y variable
y %>% 
  mutate(ycat = ifelse(RA==0 & ITA ==0, 1, 0), #no, no
         ycat = ifelse(RA==0 & ITA ==1, 2, ycat), #no, yes
         ycat = ifelse(RA==1 & ITA ==0, 3, ycat), #yes, no
         ycat = ifelse(RA==1 & ITA ==1, 4, ycat)) -> y #yes, yes

y_categories = y$ycat

# Explanatory data analysis (EDA)

# Combining x and y to data frame
all_data = data.frame(x,y_categories)

# Frequencies RA
table(y$RA)
round(table(y$RA)/length(y$RA),3)

# Frequencies ITA
table(y$ITA)
round(table(y$ITA)/length(y$ITA),3)

# 4 groups
abs_freq = data.frame(table(as.factor(y_categories)))
colnames(abs_freq) = c("Groups", "Freq")
abs_freq

# Relative frequencies of classes in %
round((table(all_data$y_categories)/sum(table(all_data$y_categories)))*100,2)

# Plot
abs_freq %>%
  ggplot(aes(x = Groups, y = Freq, label = Freq)) +
  geom_bar(stat="identity") + 
  ggtitle("Absolute Frequencies of Medical Groups") +
  theme(plot.title = element_text(hjust = 0.7)) + 
  theme_light() +
  xlab("Medical Groups") + 
  ylab("People in groups") + 
  ylim(0,80)
# Groups are highly unbalanced with less data

# Age and gender information
xlab_fem = paste("Female \n (N=", nrow(x[x$Gender==0,]), ")", sep="")
xlab_male = paste("Male \n (N=", nrow(x[x$Gender==1,]), ")", sep="")
aver_age_fem = mean(x$Age[x$Gender==0])
aver_age_mal = mean(x$Age[x$Gender==1])

ggplot() + 
  geom_jitter(data=x[x$Gender ==0,], aes(x = Gender, y=Age), 
              color="#F8766D", size =2.5) +
  geom_jitter(data=x[x$Gender ==1,], aes(x = Gender, y=Age), 
              color="#619CFF", size=2.5) + 
  geom_segment(aes(x = -0.3, xend = 0.3, y = mean(aver_age_fem), 
                   yend = mean(aver_age_fem), colour = "Female")) +
  geom_segment(aes(x = 0.7, xend = 1.3, y = mean(aver_age_mal), 
                   yend = mean(aver_age_mal), colour = "Male")) + 
  scale_color_manual(values = c("Female"= "#F8766D", "Male" = "#619CFF"), 
                     name ="Mean") +
  scale_x_continuous(breaks = c(0,1), label=c(xlab_fem, xlab_male)) + 
  labs(title = "Boxplot of Age by Gender", y = "Age") + 
  theme_light() 

## Confusion Matrix 
# Diabetes
confusion_matrix <- as.data.frame(table(all_data$Diabetes, 
                                        all_data$y_categories))
confusion_matrix %>% 
  mutate(Var1 = ifelse(Var1 == 1, "Yes", "No")) %>% 
  ggplot(mapping = aes(x = Var2, y = Var1)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) + 
  scale_fill_gradient(high = "lavenderblush1", low = "lavenderblush3", 
                      name="Frequency", limits=c(0,70)) + 
  ylab("Diabetes") + 
  xlab("") +
  theme_classic() + 
  theme(legend.position = "none",
        axis.title.x = element_text(vjust=-6), 
        axis.line.y = element_blank(), 
        axis.line.x = element_blank(), 
        axis.ticks.y = element_blank(),  
        axis.ticks.x = element_blank(),
        axis.text.x = element_text(size = 12, vjust = 0.5), 
        axis.text.y = element_text(size = 12),
        text = element_text(size = 14)) + 
  scale_x_discrete(position="top", 
                   labels = c("no RA & no ITA", "no RA & ITA", 
                              "RA & no ITA", "RA & ITA")) -> plot_diabetes

# Smoking
confusion_matrix <- as.data.frame(table(all_data$Ever_smoked, 
                                        all_data$y_categories))
confusion_matrix %>% 
  mutate(Var1 = ifelse(Var1 == 1, "Yes", "No")) %>% 
  ggplot(mapping = aes(x = Var2, y = Var1)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) + 
  scale_fill_gradient(high = "lightblue1", low = "lightblue3", 
                      name="Frequency", limits=c(0,70)) + 
  ylab("Smoker") + xlab("") + theme_classic() + 
  theme(legend.position = "none",
        axis.title.x = element_text(vjust=-6), 
        axis.line.y = element_blank(), 
        axis.line.x = element_blank(), 
        axis.ticks.y = element_blank(),  
        axis.ticks.x = element_blank(),
        axis.text.x = element_text(size = 12, vjust = 0.5), 
        axis.text.y = element_text(size = 12),
        text = element_text(size = 14)) + 
  scale_x_discrete(position="top", labels = c("no RA & no ITA", 
                                              "no RA & ITA", "RA & no ITA", 
                                              "RA & ITA")) -> plot_smoker

# PVD
confusion_matrix <- as.data.frame(table(all_data$PVD, all_data$y_categories))
confusion_matrix %>% 
  mutate(Var1 = ifelse(Var1 == 1, "Yes", "No")) %>% 
  ggplot(mapping = aes(x = Var2, y = Var1)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) + 
  scale_fill_gradient(high = "pink1", low = "pink3", name="Frequency", 
                      limits=c(0,70)) + 
  ylab("PVD") + xlab("") + theme_classic() + 
  theme(legend.position = "none",
        axis.title.x = element_text(vjust=-6), 
        axis.line.y = element_blank(), 
        axis.line.x = element_blank(), 
        axis.ticks.y = element_blank(),  
        axis.ticks.x = element_blank(),
        axis.text.x = element_text(size = 12, vjust = 0.5), 
        axis.text.y = element_text(size = 12),
        text = element_text(size = 14)) +  
  scale_x_discrete(position="top", 
                   labels = c("no RA & no ITA", "no RA & ITA", 
                              "RA & no ITA", "RA & ITA")) -> plot_pvd

# CVD
confusion_matrix <- as.data.frame(table(all_data$CVD, all_data$y_categories))
confusion_matrix %>% 
  mutate(Var1 = ifelse(Var1 == 1, "Yes", "No")) %>% 
  ggplot(mapping = aes(x = Var2, y = Var1)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) + 
  scale_fill_gradient(high = "darkseagreen1", low = "darkseagreen3", 
                      name="Frequency", limits=c(0,70)) + 
  ylab("CVD") + xlab("") + theme_classic() + 
  theme(legend.position = "none",
        axis.title.x = element_text(vjust=-6), 
        axis.line.y = element_blank(), 
        axis.line.x = element_blank(), 
        axis.ticks.y = element_blank(),  
        axis.ticks.x = element_blank(),
        axis.text.x = element_text(size = 12, vjust = 0.5), 
        axis.text.y = element_text(size = 12),
        text = element_text(size = 14)) + 
  scale_x_discrete(position="top", 
                   labels = c("no RA & no ITA", "no RA & ITA", "RA & no ITA", 
                              "RA & ITA")) -> plot_cvd

ggarrange(plot_diabetes, plot_smoker, plot_pvd, plot_cvd, ncol=1, nrow=4)

# Data wrangling

# Before asking the questions we need to find a proper model that fits our 
# data well
# Here, we are facing multiple problems because of the low total frequencies 
# in the categories, there are no reliable predictions possible. 
# I use multinomial logistic regression as a baseline model for the whole data set:
set.seed(50)
lr = multinom(y_categories ~ ., data = all_data)

# Predict
lr_predict = predict(lr, newdata = all_data)

# Building classification table
tab = table(pred = lr_predict, true = y_categories)
tab

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab)),3)

# Accuracy per Category 
accuracy_category = numeric(4)
for (i in 1:length(accuracy_category))
{
  accuracy_category[i] = (tab[i,i])/67
}
round((accuracy_category),3)

# Only the target variables that have the highest amount in total 
# frequencies were predicted from the model 
# Because the imbalance of the target variable, 
# we try to balance the data set by upsampling method from caret
upsample_data <- upSample(x, as.factor(y_categories))
table(upsample_data$Class)

# Multinomial logistic regression with upsampled data
set.seed(50)
lr = multinom(Class ~ ., data = upsample_data)

# Predict
lr_predict = predict(lr, newdata = upsample_data)

# Building classification table
tab = table(pred = lr_predict, true = upsample_data$Class)
tab

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab)),3)

# Accuracy per Category 
accuracy_category = numeric(4)
for (i in 1:length(accuracy_category))
{
  accuracy_category[i] = (tab[i,i])/67
}
round((accuracy_category),3)

# Since we see in both methods an increased accuracy rate in the last two categories, 
# the accuracy rates of the first two categories are decreasing a lot 
# To prevent this prediction imbalance between the categories, 
# I decided to perform two binary classifications for RA and ITA seperately instead
bin_y = y[-3]

# Ratio bin_y 
table(bin_y$RA)
table(bin_y$ITA)

# Upsample RA and ITA
up_RA = upSample(x, as.factor(bin_y$RA))
up_ITA = upSample(x, as.factor(bin_y$ITA))

# Show total frequencies
table(up_RA$Class)
table(up_ITA$Class)
# Now we have two balanced binary target variables and we perform our model fitting again

# Logistic regression
set.seed(50)
lr_RA = glm(Class ~.,family=binomial(link='logit'),data=up_RA)
lr_ITA = glm(Class ~.,family=binomial(link='logit'),data=up_ITA)

# Summary
summary(lr_RA)
summary(lr_ITA)

# Predict
lr_predict_RA = predict(lr_RA, newdata = up_RA, type = "response")
lr_predict_ITA = predict(lr_ITA, newdata = up_ITA, type = "response")

# Building classification table
tab_RA = table(pred = round(lr_predict_RA, 0), true = up_RA$Class)
tab_RA
tab_ITA = table(pred = round(lr_predict_ITA, 0), true = up_ITA$Class)
tab_ITA

# Calculating accuracy - sum of diagonal elements divided by total obs
# Precision
precision = function(confusion_matrix)
{
  TP = confusion_matrix[2,2] # True positive
  FP = confusion_matrix[1,2] # False positive
  return (TP/(TP + FP))
}

# Recall
recall = function(confusion_matrix)
{
  TP = confusion_matrix[2,2] # True positive
  FN = confusion_matrix[2,1] # False negative
  return (TP/(TP + FN))
}

# Metrics
acc_RA = (sum(diag(tab_RA))/sum(tab_RA)) # Accuracy RA
acc_ITA = (sum(diag(tab_ITA))/sum(tab_ITA)) # Accuracy ITA
prec_RA = precision(tab_RA) # Precision RA
prec_ITA = precision(tab_ITA) # Precision ITA
re_RA = recall(tab_RA) # Recall RA
re_ITA = recall(tab_ITA) # Recall ITA

# Metric data frame
metric = as.data.frame(matrix(
  c(acc_RA, prec_RA, re_RA, acc_ITA, prec_ITA, re_ITA), nrow = 3, ncol = 2))
colnames(metric) = c("RA", "ITA")
rownames(metric) = c("Accuracy", "Precision", "Recall")
metric

# Model fitting
# Building randomly generated train and test data set for x and y
set.seed(50)
ratio = 0.75

# RA
n = nrow(up_RA)
sample_number = (ratio * n)
train_ind = sample(n, sample_number)
xtrain = up_RA[-7][train_ind,]
ytrain = up_RA$Class[train_ind]
xtest = up_RA[-7][-train_ind,]
ytest = up_RA$Class[-train_ind]

# Building data frames
train_RA = data.frame(xtrain, ytrain)
test_RA = data.frame(xtest, ytest)

# ITA
n = nrow(up_ITA)
sample_number = (ratio * n)
train_ind = sample(n, as.integer(sample_number))
xtrain = up_ITA[-7][train_ind,]
ytrain = up_ITA$Class[train_ind]
xtest = up_ITA[-7][-train_ind,]
ytest = up_ITA$Class[-train_ind]

# Building data frames
train_ITA = data.frame(xtrain, ytrain)
test_ITA = data.frame(xtest, ytest)

# The data will be trained on Random Forest and Support Vector Machines
# Random Forest
rf_RA = randomForest(ytrain~., data=train_RA, ntree=300)
rf_ITA = randomForest(ytrain~., data=train_ITA, ntree=300)

# Summary 
print(rf_RA)
print(rf_ITA)

# Metrics
acc_RA = (sum(diag(rf_RA$confusion))/nrow(train_RA)) # Accuracy RA
acc_ITA = (sum(diag(rf_ITA$confusion))/nrow(train_ITA)) # Accuracy ITA
prec_RA = precision(rf_RA$confusion) # Precision RA
prec_ITA = precision(rf_ITA$confusion) # Precision ITA
re_RA = recall(rf_RA$confusion) # Recall RA
re_ITA = recall(rf_ITA$confusion) # Recall ITA

# Metric data frame
rf_metric_df1 = as.data.frame(
  matrix(c(acc_RA, prec_RA, re_RA, acc_ITA, prec_ITA, re_ITA), 
         nrow = 3, ncol = 2))
colnames(rf_metric_df1) = c("RA", "ITA")
rownames(rf_metric_df1) = c("Accuracy", "Precision", "Recall")
rf_metric_df1

# Parameter tuning
set.seed(50)
trees = c(50, 100, 200, 300, 400, 500)
nodes = c(2, 4, 5, 6, 8, 10)
tuned_rf_RA = tune(randomForest, ytrain~., data = train_RA, 
                   ranges = list(nodesize = nodes, ntree = trees))
tuned_rf_ITA = tune(randomForest, ytrain~., data = train_ITA, 
                    ranges = list(nodesize = nodes, ntree = trees))

# Evaluating the best size of trees
summary(tuned_rf_RA)
summary(tuned_rf_ITA)

# Prediction with tuned parameters
best_rf_RA = randomForest(ytrain~., data=train_RA, 
                          nodesize = tuned_rf_RA$best.parameters[1,1], 
                          ntree=tuned_rf_RA$best.parameters[1,2])
best_rf_ITA = randomForest(ytrain~., data=train_ITA, 
                           nodesize = tuned_rf_ITA$best.parameters[1,1], 
                          ntree=tuned_rf_ITA$best.parameters[1,2])

# Summary 
print(best_rf_RA)
print(best_rf_ITA)

# Metrics
acc_RA = (sum(diag(best_rf_RA$confusion))/nrow(train_RA)) # Accuracy RA
acc_ITA = (sum(diag(best_rf_ITA$confusion))/nrow(train_ITA)) # Accuracy ITA
prec_RA = precision(best_rf_RA$confusion) # Precision RA
prec_ITA = precision(best_rf_ITA$confusion) # Precision ITA
re_RA = recall(best_rf_RA$confusion) # Recall RA
re_ITA = recall(best_rf_ITA$confusion) # Recall ITA

# Metric data frame
rf_metric_df2 = as.data.frame(
  matrix(c(acc_RA, prec_RA, re_RA, acc_ITA, prec_ITA, re_ITA), 
         nrow = 3, ncol = 2))
colnames(rf_metric_df2) = c("RA", "ITA")
rownames(rf_metric_df2) = c("Accuracy", "Precision", "Recall")
rf_metric_df2

# SVM
set.seed(50)
svmfit_RA = svm(ytrain~., data=train_RA, cost = 10, kernel = "radial")
svmfit_ITA = svm(ytrain~., data=train_ITA, cost = 10, kernel = "radial")

# Prediction
pred_RA = predict(svmfit_RA, new_data = train_RA$ytrain)
pred_ITA = predict(svmfit_ITA, new_data = train_ITA$ytrain)

# Confusion matrix
conf_RA = table(pred = pred_RA, true = train_RA$ytrain)
conf_ITA = table(pred = pred_ITA, true = train_ITA$ytrain)

# Metrics
acc_RA = (sum(diag(conf_RA))/nrow(train_RA)) # Accuracy RA
acc_ITA = (sum(diag(conf_ITA))/nrow(train_ITA)) # Accuracy ITA
prec_RA = precision(conf_RA) # Precision RA
prec_ITA = precision(conf_ITA) # Precision ITA
re_RA = recall(conf_RA) # Recall RA
re_ITA = recall(conf_ITA) # Recall ITA

# Metric data frame
svm_metric_df1 = as.data.frame(
  matrix(c(acc_RA, prec_RA, re_RA, acc_ITA, prec_ITA, re_ITA), 
         nrow = 3, ncol = 2))
colnames(svm_metric_df1) = c("RA", "ITA")
rownames(svm_metric_df1) = c("Accuracy", "Precision", "Recall")
svm_metric_df1

# SVM tuning
# Gamma and cost values
set.seed(50)
gamma_values = c(0.1, 1, 10, 100, 1000)
cost_values = c(0.5, 1, 2, 3, 4)

# Tuning
tune_RA = tune(svm, ytrain~., data = train_RA, 
               ranges = list(gamma = gamma_values, cost = cost_values), 
               kernel = 'radial')
tune_ITA = tune(svm, ytrain~., data = train_ITA, 
                ranges = list(gamma = gamma_values, cost = cost_values), 
                kernel = 'radial')

# Summary
summary(tune_RA)
summary(tune_ITA)

# SVM with best parameter
best_RA = svm(ytrain~., data=train_RA, 
              gamma = tune_RA$best.parameters[1], 
              cost = tune_RA$best.parameters[2], kernel = "radial")
best_ITA = svm(ytrain~., data=train_ITA, 
               gamma = tune_ITA$best.parameters[1], 
               cost = tune_ITA$best.parameters[2], kernel = "radial")

# Prediction
pred_best_RA = predict(best_RA, new_data = train_RA$ytrain)
pred_best_ITA = predict(best_ITA, new_data = train_ITA$ytrain)

# Confusion matrix
conf_RA = table(pred = pred_best_RA, true = train_RA$ytrain)
conf_ITA = table(pred = pred_best_ITA, true = train_ITA$ytrain)

# Metrics
acc_RA = (sum(diag(conf_RA))/nrow(train_RA)) # Accuracy RA
acc_ITA = (sum(diag(conf_ITA))/nrow(train_ITA)) # Accuracy ITA
prec_RA = precision(conf_RA) # Precision RA
prec_ITA = precision(conf_ITA) # Precision ITA
re_RA = recall(conf_RA) # Recall RA
re_ITA = recall(conf_ITA) # Recall ITA

# Metric data frame
best_svm_metric_df = as.data.frame(
  matrix(c(acc_RA, prec_RA, re_RA, acc_ITA, prec_ITA, re_ITA), 
         nrow = 3, ncol = 2))
colnames(best_svm_metric_df) = c("RA", "ITA")
rownames(best_svm_metric_df) = c("Accuracy", "Precision", "Recall")
best_svm_metric_df

# Model Selection

# SVM outperforms Random Forest on the training sample, 
# but a high training accuracy doesnt indicate a high testing accuracy
# Performing Cross-Validation for Model Selection with best models
# Random Forest CV
set.seed(50)
trControl <- trainControl(method = "cv", number = 5, search = "grid")

# RA
mtry <- sqrt(ncol(train_RA))
tunegrid <- expand.grid(.mtry=mtry)
rf_RA_cv <- train(ytrain~., data = train_RA, method = "rf", 
                  metric = "Accuracy", tuneGrid = tunegrid,
                  trControl = trControl, 
                  nodesize = tuned_rf_RA$best.parameters[1,1],
                  ntree = tuned_rf_RA$best.parameters[1,2])

# ITA
mtry <- sqrt(ncol(train_ITA))
tunegrid <- expand.grid(.mtry=mtry)
rf_ITA_cv = train(ytrain~., data = train_ITA, method = "rf", 
                  metric = "Accuracy", tuneGrid = tunegrid,
                  trControl = trControl, 
                  nodesize = tuned_rf_ITA$best.parameters[1,1],
                  ntree = tuned_rf_ITA$best.parameters[1,2])

# Average accuracy 
print(rf_RA_cv)
print(rf_ITA_cv)

# SVM CV
set.seed(50)
n1 = dim(train_RA)[1]
n2 = dim(train_ITA)[1]
cvvec_RA <- sample(rep(1:5, length.out = n1, replace = TRUE))
cvvec_ITA = sample(rep(1:5, length.out = n2, replace = TRUE))
pred_y_RA = numeric(n1)
pred_y_ITA = numeric(n2)
mat = matrix(0, 6, 2)

# Self build cross validation
for (i in 1:5) 
{
  # Model
  best_RA = svm(ytrain[cvvec_RA != i]~., data=train_RA[cvvec_RA != i,], 
                gamma = tune_RA$best.parameters[1], 
                cost = tune_RA$best.parameters[2], kernel = "radial")
  best_ITA = svm(ytrain[cvvec_ITA != i]~., data=train_ITA[cvvec_ITA != i,], 
                gamma = tune_ITA$best.parameters[1], 
                cost = tune_ITA$best.parameters[2], kernel = "radial")
  # Predict
  pred_y_RA[cvvec_RA == i] <- predict(best_RA, 
                                      newdata = train_RA[cvvec_RA == i,])
  pred_y_ITA[cvvec_ITA == i] <- predict(best_ITA, 
                                      newdata = train_ITA[cvvec_ITA == i,])
  # Confusion Matrix
  tab_RA = table(pred = as.integer(pred_y_RA[cvvec_RA == i]), 
                 true = train_RA$ytrain[cvvec_RA == i])
  tab_ITA = table(pred = as.integer(pred_y_ITA[cvvec_ITA == i]), 
                  true = train_ITA$ytrain[cvvec_ITA == i])
  # Accuracy matrix RA & ITA
  mat[i, 1] = round((sum(diag(tab_RA))/sum(tab_RA)),2)
  mat[i, 2] = round((sum(diag(tab_ITA))/sum(tab_ITA)),2)
}

# Accuracy result as data frame
mat = as.data.frame(mat)
colnames(mat) = c("RA", "ITA")
rownames(mat) = c(1:5, "Average")
mat[6,1] = mean(mat[1:5,1])
mat[6,2] = mean(mat[1:5,2])
mat

# Fitting on test data
# Prediction with tuned parameters
set.seed(50)
pred_RA = predict(best_rf_RA, newdata = test_RA[-7])
pred_ITA = predict(best_rf_ITA, newdata = test_ITA[-7])

# Confusion matrix
conf_RA = table(pred = pred_RA, true = test_RA$ytest)
conf_RA
conf_ITA = table(pred = pred_ITA, true = test_ITA$ytest)
conf_ITA

# Metrics
acc_RA = (sum(diag(conf_RA))/nrow(test_RA)) # Accuracy RA
acc_ITA = (sum(diag(conf_ITA))/nrow(test_ITA)) # Accuracy ITA
prec_RA = precision(conf_RA) # Precision RA
prec_ITA = precision(conf_ITA) # Precision ITA
re_RA = recall(conf_RA) # Recall RA
re_ITA = recall(conf_ITA) # Recall ITA

# Metric data frame
final_model = as.data.frame(
  matrix(c(acc_RA, prec_RA, re_RA, acc_ITA, prec_ITA, re_ITA), 
         nrow = 3, ncol = 2))
colnames(final_model) = c("RA", "ITA")
rownames(final_model) = c("Accuracy", "Precision", "Recall")
final_model
```


